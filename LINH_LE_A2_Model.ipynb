{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Score: 0.807\n",
      "AUC Score: 0.8038\n"
     ]
    }
   ],
   "source": [
    "# timeit\n",
    "\n",
    "# Student Name :  Linh Le\n",
    "# Cohort       :  3\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Import Packages\n",
    "################################################################################\n",
    "\n",
    "# importing necessary libraries\n",
    "import pandas as pd                                     # data science essentials\n",
    "import seaborn as sns                                   # essential graphical output\n",
    "import matplotlib.pyplot as plt                         # enhanced graphical output\n",
    "import statsmodels.formula.api as smf                   # regression modeling\n",
    "from sklearn.model_selection import train_test_split    # train/test split\n",
    "import sklearn.linear_model                             # linear models (scikit-learn)\n",
    "from sklearn.metrics import confusion_matrix            # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score               # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier      # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor       # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler        # standard scaler\n",
    "\n",
    "# CART model packages\n",
    "from sklearn.tree import DecisionTreeRegressor          # Regression trees     \n",
    "from sklearn.tree import DecisionTreeClassifier         # Classification trees\n",
    "from sklearn.tree import export_graphviz                # exports graphics\n",
    "from sklearn.externals.six import StringIO              # save object in memory\n",
    "from IPython.display import Image                       # displays an image on the frontend\n",
    "import pydotplus\n",
    "\n",
    "# Hyper parameter tuning and ensemble modeling\n",
    "from sklearn.model_selection import GridSearchCV        # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                 # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "from sklearn.naive_bayes import GaussianNB              # Gaussian Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Load Data\n",
    "################################################################################\n",
    "\n",
    "original_df = pd.read_excel(\"Apprentice_Chef_Dataset.xlsx\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Feature Engineering and (optional) Dataset Standardization\n",
    "################################################################################\n",
    "\n",
    "# Looping over columns with missing values\n",
    "for col in original_df:\n",
    "\n",
    "    # creating columns with 1s if missing and 0 if not\n",
    "    if original_df[col].isnull().astype(int).sum() > 0:\n",
    "        original_df['m_'+col] = original_df[col].isnull().astype(int)\n",
    "        \n",
    "\n",
    "# Creating an imputation value (through soft-coding)\n",
    "fill = \"Unknown\"\n",
    "\n",
    "# Imputing 'FAMILY_NAME'\n",
    "original_df['FAMILY_NAME'] = original_df['FAMILY_NAME'].fillna(fill)\n",
    "\n",
    "################################################################################\n",
    "# Outlier Thresholds\n",
    "\n",
    "# Creating a comprehensive list of variables\n",
    "\n",
    "all_variables =['REVENUE',\n",
    "                'CROSS_SELL_SUCCESS', \n",
    "                'TOTAL_MEALS_ORDERED', \n",
    "                'UNIQUE_MEALS_PURCH',\n",
    "                'CONTACTS_W_CUSTOMER_SERVICE', \n",
    "                'PRODUCT_CATEGORIES_VIEWED',\n",
    "                'AVG_TIME_PER_SITE_VISIT', \n",
    "                'MOBILE_NUMBER', \n",
    "                'CANCELLATIONS_BEFORE_NOON',\n",
    "                'CANCELLATIONS_AFTER_NOON', \n",
    "                'TASTES_AND_PREFERENCES', \n",
    "                'MOBILE_LOGINS',\n",
    "                'PC_LOGINS', \n",
    "                'WEEKLY_PLAN', \n",
    "                'EARLY_DELIVERIES', \n",
    "                'LATE_DELIVERIES',\n",
    "                'PACKAGE_LOCKER', \n",
    "                'REFRIGERATED_LOCKER', \n",
    "                'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "                'AVG_PREP_VID_TIME', \n",
    "                'LARGEST_ORDER_SIZE', \n",
    "                'MASTER_CLASSES_ATTENDED',\n",
    "                'MEDIAN_MEAL_RATING', \n",
    "                'AVG_CLICKS_PER_VISIT', \n",
    "                'TOTAL_PHOTOS_VIEWED']\n",
    "\n",
    "\n",
    "# Creating a list of explanatory variables (x variables)\n",
    "\n",
    "x_variables =['REVENUE', \n",
    "              'TOTAL_MEALS_ORDERED', \n",
    "              'UNIQUE_MEALS_PURCH',\n",
    "              'CONTACTS_W_CUSTOMER_SERVICE', \n",
    "              'PRODUCT_CATEGORIES_VIEWED',\n",
    "              'AVG_TIME_PER_SITE_VISIT', \n",
    "              'MOBILE_NUMBER', \n",
    "              'CANCELLATIONS_BEFORE_NOON',\n",
    "              'CANCELLATIONS_AFTER_NOON', \n",
    "              'TASTES_AND_PREFERENCES', \n",
    "              'MOBILE_LOGINS',\n",
    "              'PC_LOGINS', \n",
    "              'WEEKLY_PLAN', \n",
    "              'EARLY_DELIVERIES', \n",
    "              'LATE_DELIVERIES',\n",
    "              'PACKAGE_LOCKER', \n",
    "              'REFRIGERATED_LOCKER', \n",
    "              'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "              'AVG_PREP_VID_TIME', \n",
    "              'LARGEST_ORDER_SIZE', \n",
    "              'MASTER_CLASSES_ATTENDED',\n",
    "              'MEDIAN_MEAL_RATING', \n",
    "              'AVG_CLICKS_PER_VISIT', \n",
    "              'TOTAL_PHOTOS_VIEWED']\n",
    "\n",
    "\n",
    "# Setting outlier thresholds based on histograms\n",
    "\n",
    "total_meals_hi  = 180          # data starts to become skewed after this point\n",
    "           \n",
    "unique_meals_lo = 1.5          # no data points below this point\n",
    "unique_meals_hi = 9            # there's a sharp drop at 10\n",
    "\n",
    "contacts_cust_lo = 3           # there's a drop below 3\n",
    "contacts_cust_hi = 10          # uncharacteristic increase after 10 that does not follow normal distribution\n",
    "\n",
    "prod_viewed_lo = 1             # no data points below 1\n",
    "prod_viewed_hi = 10            # no data points above 10\n",
    "\n",
    "avg_site_time_hi = 175         # few customers spend over 175 seconds on the website\n",
    "\n",
    "canc_before_noon_hi = 5        # data skews after 5\n",
    " \n",
    "canc_after_noon_lo = 1         # very few data points below this number\n",
    "canc_after_noon_hi = 2         # very few data points after this number\n",
    "\n",
    "pc_logins_lo = 5               # very few data points below 5\n",
    "pc_logins_hi = 6               # very few data points above 6\n",
    "\n",
    "mobile_logins_lo = 1           # very few data points below 1\n",
    "mobile_logins_hi = 2           # very few data points above 2\n",
    "\n",
    "weekly_plan_hi = 14            # sharp drop after 14\n",
    "\n",
    "early_del_hi = 4               # sharp drop after 4\n",
    "\n",
    "late_del_hi = 7                # data skewed after this point\n",
    "\n",
    "avg_prep_time_lo = 80          # few points below 80\n",
    "avg_prep_time_hi = 230         # data skews after this point\n",
    "\n",
    "largest_order_lo = 2           # sharp drop below this point\n",
    "largest_order_hi = 7           # sharp drop after this point\n",
    "\n",
    "master_class_hi = 1            # more than 1 class is an outlier\n",
    "\n",
    "median_rating_lo = 2           # few points below 2\n",
    "median_rating_hi = 4           # very few points after 4\n",
    "\n",
    "avg_clicks_lo = 8              # few points below 8\n",
    "avg_clicks_hi = 17.5           # few points above 17.5\n",
    "\n",
    "\n",
    "revenue_lo = 500               # few values below 500\n",
    "revenue_hi = 2500              # small uncharacteristic rise after this point\n",
    "\n",
    "\n",
    "# Setting multiple outlier thresholds for FOLLOWED_RECOMMEMDATIONS_PCT \n",
    "#(which was found to be highly correlated to CROSS_SELL_SUCCESS)\n",
    "\n",
    "followed_rec_1 = 30\n",
    "followed_rec_2 = 60\n",
    "followed_rec_3 = 80\n",
    "\n",
    "\n",
    "\n",
    "# Developing features (columns) for outliers based on previously-defined thresholds\n",
    "\n",
    "# Total Meals\n",
    "original_df['out_total_meals'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_total_meals'][original_df['TOTAL_MEALS_ORDERED'] > total_meals_hi]\n",
    "\n",
    "original_df['out_total_meals'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Unique Meals Purchased\n",
    "original_df['out_unique_meals'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_unique_meals'][original_df['UNIQUE_MEALS_PURCH'] > unique_meals_hi]\n",
    "condition_lo = original_df.loc[0:,'out_unique_meals'][original_df['UNIQUE_MEALS_PURCH'] < unique_meals_lo]\n",
    "\n",
    "original_df['out_unique_meals'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_unique_meals'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Contacts with Customer Service\n",
    "original_df['out_contacts_cust'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_contacts_cust'][original_df['CONTACTS_W_CUSTOMER_SERVICE'] > contacts_cust_hi]\n",
    "condition_lo = original_df.loc[0:,'out_contacts_cust'][original_df['CONTACTS_W_CUSTOMER_SERVICE'] < contacts_cust_lo]\n",
    "\n",
    "original_df['out_contacts_cust'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_contacts_cust'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Product Categories Viewed\n",
    "original_df['out_prod_viewed'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_prod_viewed'][original_df['PRODUCT_CATEGORIES_VIEWED'] > prod_viewed_hi]\n",
    "condition_lo = original_df.loc[0:,'out_prod_viewed'][original_df['PRODUCT_CATEGORIES_VIEWED'] < prod_viewed_lo]\n",
    "\n",
    "original_df['out_prod_viewed'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_prod_viewed'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Average Time per Site Visit\n",
    "original_df['out_avg_site_time'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_avg_site_time'][original_df['AVG_TIME_PER_SITE_VISIT'] > avg_site_time_hi]\n",
    "\n",
    "original_df['out_avg_site_time'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Cancellations Before Noon\n",
    "original_df['out_canc_before_noon'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_canc_before_noon'][original_df['CANCELLATIONS_BEFORE_NOON'] > canc_before_noon_hi]\n",
    "\n",
    "original_df['out_canc_before_noon'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Cancellations After Noon\n",
    "original_df['out_canc_after_noon'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_canc_after_noon'][original_df['CANCELLATIONS_AFTER_NOON'] > canc_after_noon_hi]\n",
    "condition_lo = original_df.loc[0:,'out_canc_after_noon'][original_df['CANCELLATIONS_AFTER_NOON'] < canc_after_noon_hi]\n",
    "\n",
    "original_df['out_canc_after_noon'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "original_df['out_canc_after_noon'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# PC Logins\n",
    "original_df['out_pc_logins'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_pc_logins'][original_df['PC_LOGINS'] > pc_logins_hi]\n",
    "condition_lo = original_df.loc[0:,'out_pc_logins'][original_df['PC_LOGINS'] < pc_logins_lo]\n",
    "\n",
    "original_df['out_pc_logins'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_pc_logins'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Mobile Logins\n",
    "original_df['out_mobile_logins'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_mobile_logins'][original_df['MOBILE_LOGINS'] > mobile_logins_hi]\n",
    "condition_lo = original_df.loc[0:,'out_mobile_logins'][original_df['MOBILE_LOGINS'] < mobile_logins_lo]\n",
    "\n",
    "original_df['out_mobile_logins'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_mobile_logins'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Weekly Plan\n",
    "original_df['out_weekly_plan'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_weekly_plan'][original_df['WEEKLY_PLAN'] > weekly_plan_hi]\n",
    "\n",
    "original_df['out_weekly_plan'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Early Deliveries\n",
    "original_df['out_early_deliveries'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_early_deliveries'][original_df['EARLY_DELIVERIES'] > early_del_hi]\n",
    "\n",
    "original_df['out_early_deliveries'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "# Late Deliveries\n",
    "original_df['out_late_deliveries'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_late_deliveries'][original_df['LATE_DELIVERIES'] > late_del_hi]\n",
    "\n",
    "original_df['out_late_deliveries'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Average Preparation Video Time\n",
    "original_df['out_avg_prep_vid_time'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_avg_prep_vid_time'][original_df['AVG_PREP_VID_TIME'] > avg_prep_time_hi]\n",
    "condition_lo = original_df.loc[0:,'out_avg_prep_vid_time'][original_df['AVG_PREP_VID_TIME'] < avg_prep_time_lo]\n",
    "\n",
    "original_df['out_avg_prep_vid_time'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_avg_prep_vid_time'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "# Largest Order\n",
    "original_df['out_largest_order'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_largest_order'][original_df['LARGEST_ORDER_SIZE'] > largest_order_hi]\n",
    "condition_lo = original_df.loc[0:,'out_largest_order'][original_df['LARGEST_ORDER_SIZE'] < largest_order_lo]\n",
    "\n",
    "original_df['out_largest_order'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_largest_order'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "\n",
    "# Master Classes Attended\n",
    "original_df['out_master_classes'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_master_classes'][original_df['MASTER_CLASSES_ATTENDED'] > master_class_hi]\n",
    "\n",
    "original_df['out_master_classes'].replace(to_replace = condition_hi,\n",
    "                                value      = 1,\n",
    "                                inplace    = True)\n",
    "\n",
    "\n",
    "# Median Meal Rating\n",
    "original_df['out_median_rating'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_median_rating'][original_df['MEDIAN_MEAL_RATING'] > median_rating_hi]\n",
    "condition_lo = original_df.loc[0:,'out_median_rating'][original_df['MEDIAN_MEAL_RATING'] < median_rating_lo]\n",
    "\n",
    "original_df['out_median_rating'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_median_rating'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "# Average Clicks per Visit\n",
    "original_df['out_avg_clicks'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_avg_clicks'][original_df['AVG_CLICKS_PER_VISIT'] > avg_clicks_hi]\n",
    "condition_lo = original_df.loc[0:,'out_avg_clicks'][original_df['AVG_CLICKS_PER_VISIT'] < avg_clicks_lo]\n",
    "\n",
    "original_df['out_avg_clicks'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_avg_clicks'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "# Revenue\n",
    "original_df['out_revenue'] = 0\n",
    "condition_hi = original_df.loc[0:,'out_revenue'][original_df['REVENUE'] > revenue_hi]\n",
    "condition_lo = original_df.loc[0:,'out_revenue'][original_df['REVENUE'] < revenue_lo]\n",
    "\n",
    "original_df['out_revenue'].replace(to_replace = condition_hi,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "original_df['out_revenue'].replace(to_replace = condition_lo,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "#######\n",
    "\n",
    "original_df['out_followed_rec_1'] = 0\n",
    "condition_1 = original_df.loc[0:,'out_followed_rec_1'][original_df['FOLLOWED_RECOMMENDATIONS_PCT'] > followed_rec_1]\n",
    "\n",
    "original_df['out_followed_rec_1'].replace(to_replace = condition_1,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "original_df['out_followed_rec_2'] = 0\n",
    "condition_2 = original_df.loc[0:,'out_followed_rec_2'][original_df['FOLLOWED_RECOMMENDATIONS_PCT'] > followed_rec_2]\n",
    "\n",
    "\n",
    "original_df['out_followed_rec_2'].replace(to_replace = condition_2,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "\n",
    "original_df['out_followed_rec_3'] = 0\n",
    "condition_3 = original_df.loc[0:,'out_followed_rec_3'][original_df['FOLLOWED_RECOMMENDATIONS_PCT'] > followed_rec_3]\n",
    "\n",
    "original_df['out_followed_rec_3'].replace(to_replace = condition_3,\n",
    "                                    value      = 1,\n",
    "                                    inplace    = True)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Splitting emails\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []     \n",
    "\n",
    "# looping over each email address\n",
    "for index, col in original_df.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = original_df.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "\n",
    "# displaying the results\n",
    "email_df\n",
    "\n",
    "\n",
    "# Concatenating with original DataFrame \n",
    "\n",
    "# Renaming column to concatenate\n",
    "email_df.columns = ['name' , 'email_domain']     # Renaming columns 0 and 1 from before to \"name\" and \"domain\"\n",
    "\n",
    "\n",
    "# Concatenating email_domain with original_df DataFrame\n",
    "original_df = pd.concat([original_df, email_df['email_domain']],\n",
    "                   axis = 1)\n",
    "\n",
    "\n",
    "# Printing value counts of email_domain\n",
    "original_df.loc[: ,'email_domain'].value_counts()\n",
    "\n",
    "\n",
    "# Aggregating domains into higher-level categories\n",
    "\n",
    "# Email domain types\n",
    "professional_email_domains = ['@mmm.com',\n",
    "                              '@amex.com',\n",
    "                              '@apple.com',\n",
    "                              '@boeing.com',\n",
    "                              '@caterpillar.com',\n",
    "                              '@chevron.com',\n",
    "                              '@cisco.com',\n",
    "                              '@cocacola.com',\n",
    "                              '@disney.com',\n",
    "                              '@dupont.com',\n",
    "                              '@exxon.com',\n",
    "                              '@ge.org',\n",
    "                              '@goldmansacs.com',\n",
    "                              '@homedepot.com',\n",
    "                              '@ibm.com',\n",
    "                              '@intel.com',\n",
    "                              '@jnj.com',\n",
    "                              '@jpmorgan.com',\n",
    "                              '@mcdonalds.com',\n",
    "                              '@merck.com',\n",
    "                              '@microsoft.com',\n",
    "                              '@nike.com',\n",
    "                              '@pfizer.com',\n",
    "                              '@pg.com',\n",
    "                              '@travelers.com',\n",
    "                              '@unitedtech.com',\n",
    "                              '@unitedhealth.com',\n",
    "                              '@verizon.com',\n",
    "                              '@visa.com',\n",
    "                              '@walmart.com']\n",
    " \n",
    "            \n",
    "personal_email_domains = ['@gmail.com', \n",
    "                          '@yahoo.com',\n",
    "                          '@protonmail.com']\n",
    "\n",
    "\n",
    "junk_email_domains  = ['@me.com',\n",
    "                       '@aol.com',\n",
    "                       '@hotmail.com',\n",
    "                       '@live.com',\n",
    "                       '@msn.com',\n",
    "                       '@passport.com']\n",
    "\n",
    "\n",
    "# Placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# Looping to group observations by domain type\n",
    "for domain in original_df['email_domain']:\n",
    "        if '@' + domain in professional_email_domains:\n",
    "            placeholder_lst.append('professional')\n",
    "    \n",
    "        elif '@' + domain in personal_email_domains:\n",
    "            placeholder_lst.append('personal')\n",
    "            \n",
    "        elif '@' + domain in junk_email_domains:\n",
    "            placeholder_lst.append('junk')\n",
    "            \n",
    "        else:\n",
    "            print('Unknown')\n",
    "\n",
    "\n",
    "# Concatenating with original DataFrame\n",
    "original_df['domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# Checking results\n",
    "original_df['domain_group'].value_counts()\n",
    "\n",
    "\n",
    "# One hot encoding emails\n",
    "one_hot_domain_group = pd.get_dummies(original_df['domain_group'])\n",
    "\n",
    "# Joining encoded variables with dataset\n",
    "original_df = original_df.join([one_hot_domain_group])\n",
    "\n",
    "\n",
    "# Dropping categorical variables after they've been encoded\n",
    "\n",
    "original_df_dropped = original_df.drop(['NAME', \n",
    "                          'FIRST_NAME', \n",
    "                          'FAMILY_NAME',\n",
    "                          'EMAIL', \n",
    "                          'email_domain', \n",
    "                          'domain_group', \n",
    "                          'CROSS_SELL_SUCCESS'],\n",
    "               axis = 1)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Model variables identified based on logistic regression (p-value less than 0.05)\n",
    "model_4 = ['MOBILE_NUMBER',\n",
    "           'CANCELLATIONS_BEFORE_NOON',\n",
    "           'CANCELLATIONS_AFTER_NOON',\n",
    "           'TASTES_AND_PREFERENCES',\n",
    "           'PC_LOGINS',\n",
    "           'MOBILE_LOGINS',\n",
    "           'REFRIGERATED_LOCKER',\n",
    "           'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "           'm_FAMILY_NAME',\n",
    "           'personal',\n",
    "           'professional']\n",
    "\n",
    "# Assigning only explanatory variables to a new dataframe for scaling\n",
    "\n",
    "original_df_dropped_df = original_df_dropped.loc[ : , model_4]\n",
    "\n",
    "\n",
    "### Scaling data\n",
    "\n",
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the scaler with original_df data with all the strings and categorical variables dropped\n",
    "# Only want to standardize the X side, because we only have one y variable so the variance would be all the same\n",
    "scaler.fit(original_df_dropped_df)   \n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(original_df_dropped_df)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "\n",
    "# Renaming scaled data columns from numbers to original names\n",
    "X_scaled_df.columns = original_df_dropped_df.columns\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Train/Test Split\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Explanatory variable data\n",
    "original_df_explanatory = X_scaled_df\n",
    "\n",
    "\n",
    "# Response variable data\n",
    "original_df_target = original_df.loc[:, 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# Preparing training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                   original_df_explanatory,\n",
    "                                   original_df_target,\n",
    "                                   test_size = 0.25,\n",
    "                                   random_state = 222,\n",
    "                                   stratify = original_df_target)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Final Model (instantiate, fit, and predict) -- GaussianNB\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object\n",
    "gaus_model = GaussianNB()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "gaus_model.fit(X_train, y_train)\n",
    "\n",
    "# PREDICTING on new data\n",
    "gaus_pred = gaus_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Final Model Score (score)\n",
    "################################################################################\n",
    "\n",
    "test_score = gaus_model.score(X_test, y_test).round(3)\n",
    "\n",
    "auc_score = roc_auc_score(y_true  = y_test,\n",
    "                          y_score = gaus_pred).round(4)\n",
    "\n",
    "\n",
    "print('Testing Score:', test_score)\n",
    "print('AUC Score:',     auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
